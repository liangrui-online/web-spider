import re

import requests
from lxml import etree
from faker import Factory

from dump_and_load import *
from utils import *

fc = Factory.create()
base_url = "http://www.a-hospital.com"


def load_html_with_lxml(func):
    def wrapper(*args, **kwargs):
        html_text = func(*args, **kwargs)
        return etree.HTML(html_text)

    return wrapper


# ä¸‹è½½ç½‘é¡µ
@load_html_with_lxml
@cache
def download_page(url) -> str:
    if "index.php" in url:
        raise requests.exceptions.HTTPError()
    headers = {"User-Agent": fc.user_agent()}
    r = requests.get(url, headers=headers)
    r.raise_for_status()
    return r.text


# è·å–çœä»½åˆ—è¡¨
def get_province_list(home_page_url="http://www.a-hospital.com/w/å…¨å›½åŒ»é™¢åˆ—è¡¨") -> dict:
    try:
        page = download_page(home_page_url)
    except requests.exceptions.HTTPError as e:
        logger.warning(f"é¡µé¢æ— æ³•è®¿é—®ï¼š{url_decode(home_page_url)}")
        return {}

    pvc = page.xpath('//*[@id="bodyContent"]/h3/span/text()') + page.xpath(
        '//*[@id="bodyContent"]/h3/span/a/text()'
    )
    links = page.xpath('//*[@id="bodyContent"]/p/b/a') + page.xpath(
        '//*[@id="bodyContent"]/h3/span/a'
    )
    province_name_list = list(filter(lambda x: x, map(remove_suffix, pvc)))
    province_link_list = [base_url + p.get("href") for p in links]
    return dict(zip(province_name_list, province_link_list))


# è·å–å¸‚çº§åˆ—è¡¨
def get_city_list(province_detail_page_url) -> dict:
    try:
        page = download_page(province_detail_page_url)
    except requests.exceptions.HTTPError as e:
        logger.warning(f"é¡µé¢æ— æ³•è®¿é—®ï¼š{url_decode(province_detail_page_url)}")
        return {}

    city_link = page.xpath('//*[@id="bodyContent"]/p[2]/a')
    return {i.text: base_url + i.get("href") for i in city_link}


# è·å–å¿çº§åˆ—è¡¨
def get_county_list(city_detail_page_url) -> dict:
    try:
        page = download_page(city_detail_page_url)
    except requests.exceptions.HTTPError as e:
        logger.warning(f"é¡µé¢æ— æ³•è®¿é—®ï¼š{url_decode(city_detail_page_url)}")
        return {}

    city_link = page.xpath('//*[@id="bodyContent"]/ul[1]/li/a')
    return {
        remove_suffix(i.text, "åŒ»é™¢").replace(" ", ""): base_url + i.get("href")
        for i in city_link
    }


# ä»å¿çº§åˆ«è¯¦æƒ…é¡µè·å–åŒ»é™¢ä¿¡æ¯
def parse_hospitals_from_county_page(county_page_url) -> list:
    hospitals = []

    try:
        county_page = download_page(county_page_url)
    except requests.exceptions.HTTPError as e:
        logger.warning(f"é¡µé¢æ— æ³•è®¿é—®ï¼š{url_decode(county_page_url)}")
        return hospitals

    province_name = county_page.xpath('//*[@class="nav"][1]//a[3]/text()')[0]
    province_name = re.sub(r"(åŒ»é™¢åˆ—è¡¨)", "", province_name, count=1)

    try:
        city_name = county_page.xpath('//*[@class="nav"][1]//a[4]/text()')[0]
        city_name = re.sub(r"(åŒ»é™¢åˆ—è¡¨)", "", city_name, count=1)
    except IndexError:
        city_name = county_page.xpath('//*[@class="nav"][1]//td/text()[last()]')[0]
        city_name = re.sub(f"[(åŒ»é™¢åˆ—è¡¨)({province_name}) >]", "", city_name)

    try:
        county_name = county_page.xpath('//*[@class="nav"][1]//td/text()[last()]')[0]
        county_name = re.sub(
            f"[(åŒ»é™¢åˆ—è¡¨)({province_name})({city_name}) >]", "", county_name
        )
    except IndexError:
        county_name = ""

    for item in county_page.xpath('//*[@id="bodyContent"]/ul[last()-1]/li'):
        # print(url_decode(etree.tostring(item, encoding="unicode", pretty_print=True)))
        hospital_title_with_link = item.xpath("./b/a")
        if hospital_title_with_link:
            hospital_name = hospital_title_with_link[0].text + "".join(
                item.xpath("./b/text()")
            )
            hospital_url = url_decode(hospital_title_with_link[0].get("href"))
        else:
            # æ— é”¡å¸‚æ»¨æ¹–åŒºè£å··åŒ»é™¢?æ— é”¡å’Œå¹³éª¨ç§‘åŒ»é™¢
            hospital_name = "".join(item.xpath("./b/text()"))
            hospital_url = ""

        hospital_info = {
            "åŒ»é™¢åç§°": hospital_name,
            "åŒ»é™¢ä¸»é¡µ": base_url + hospital_url,
            "çœçº§è¡Œæ”¿åŒº": province_name,
            "åœ°çº§è¡Œæ”¿åŒº": city_name,
            "å¿çº§è¡Œæ”¿åŒº": county_name,
        }
        for attr in item.xpath("./ul[1]/li"):
            attr_name = attr.xpath("./b/text()")[0]
            attr_value = "ã€".join(
                list(
                    filter(
                        lambda x: x and x not in ("ï¼š", "ã€"),
                        map(
                            lambda s: s.replace(" ", "")
                            .replace("ï¼š", "")
                            .replace("ã€", ""),
                            attr.xpath("./text()") + attr.xpath("./a/text()"),
                        ),
                    )
                )
            )
            hospital_info[attr_name] = attr_value
        # hospital_info_words = list(filter(lambda x: x and x != "ï¼š", map(lambda s: s.strip().strip("ï¼š"), (
        #         item.xpath("./ul[1]/li/b/text()") +
        #         item.xpath("./ul[1]/li/text()") +
        #         item.xpath("./ul[1]/li/a[1]/text()")
        # ))))
        #
        # key_num = int(len(hospital_info_words) / 2)
        # for i in range(key_num):
        #     hospital_info[hospital_info_words[i]] = hospital_info_words[i + key_num]
        hospitals.append(hospital_info)
    return hospitals


@logger.catch
@dump_data
def get_hospital_list():
    hospitals = []
    for p_name, p_link in get_province_list().items():
        logger.info(f"ã€{p_name}ã€‘ å¼€å§‹ä¸‹è½½ âš¡ï¸âš¡ï¸âš¡ï¸")
        for c_name, c_link in get_city_list(province_detail_page_url=p_link).items():
            if p_name in (  # ç›´è¾–å¸‚çš„ç‰‡åŒºæ²¡æœ‰"å¿"
                "åŒ—äº¬å¸‚",
                "ä¸Šæµ·å¸‚",
                "é‡åº†å¸‚",
                "å¤©æ´¥å¸‚",
            ) or c_name in (  # åœ°çº§è¡Œæ”¿åŒºæ²¡æœ‰ä¸‹å±è¾–åŒº
                "å¤©é—¨å¸‚",
                "ä»™æ¡ƒå¸‚",
                "æµæºå¸‚",
                "ç¥å†œæ¶æ—åŒº",
                "æ½œæ±Ÿå¸‚",
            ):
                new_hospitals = parse_hospitals_from_county_page(county_page_url=c_link)
                logger.info(f"ã€{p_name}ã€‘ã€{c_name}ã€‘ è§£æåˆ°{len(new_hospitals)}å®¶åŒ»é™¢")
                hospitals.extend(new_hospitals)
            else:  # æ­£å¸¸çš„ çœã€å¸‚ã€å¿ ä¸‰çº§ç»“æ„
                county = get_county_list(city_detail_page_url=c_link)
                for x_name, x_link in county.items():
                    new_hospitals = parse_hospitals_from_county_page(
                        county_page_url=x_link
                    )
                    hospitals.extend(new_hospitals)
                    logger.info(f"ã€{p_name}ã€‘ã€{c_name}ã€‘ã€{x_name}ã€‘ è§£æåˆ°{len(new_hospitals)}å®¶åŒ»é™¢")
        logger.info(f"ã€{p_name}ã€‘ ä¸‹è½½å®Œæ¯• ğŸºğŸºğŸº")
    logger.info(f"åŒ»é™¢ä¿¡æ¯è·å–å®Œæˆï¼Œå…±è§£æåˆ°{len(hospitals)}å®¶åŒ»é™¢ä¿¡æ¯ã€‚")
    return hospitals
